---
title: "Starting"
description: "DÆ°á»›i Ä‘Ã¢y lÃ  má»™t bÃ i viáº¿t máº«u báº±ng tiáº¿ng Viá»‡t vá» LLMs (Large Language Models), táº­p trung vÃ o thÃ nh pháº§n ká»¹ thuáº­t."
date: 2025-09-09
order: 2
---
# ğŸ” Tá»•ng quan ká»¹ thuáº­t vá» LLMs (Large Language Models)

## 1. Giá»›i thiá»‡u chung

**Large Language Models (LLMs)** lÃ  cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn khá»‘i lÆ°á»£ng dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“ Ä‘á»ƒ **hiá»ƒu vÃ  sinh ngÃ´n ngá»¯ tá»± nhiÃªn**. Nhá»¯ng mÃ´ hÃ¬nh nhÆ° GPT, LLaMA, PaLM hay Claude hiá»‡n nay Ä‘á»u dá»±a trÃªn kiáº¿n trÃºc **Transformer** â€“ má»™t cá»™t má»‘c Ä‘á»™t phÃ¡ trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).

---

## 2. Kiáº¿n trÃºc tá»•ng thá»ƒ cá»§a LLM

### 2.1. Transformer Architecture

LLM hiá»‡n Ä‘áº¡i thÆ°á»ng dá»±a trÃªn **Transformer Decoder** hoáº·c **Encoder-Decoder**.
Kiáº¿n trÃºc Transformer gá»“m hai thÃ nh pháº§n chÃ­nh:

* **Encoder**: MÃ£ hÃ³a chuá»—i Ä‘áº§u vÃ o thÃ nh biá»ƒu diá»…n áº©n (hidden representations).
* **Decoder**: Dá»±a trÃªn biá»ƒu diá»…n nÃ y Ä‘á»ƒ sinh chuá»—i Ä‘áº§u ra, tá»«ng token má»™t.

### 2.2. ThÃ nh pháº§n cá»‘t lÃµi trong Transformer

#### ğŸ§© Multi-Head Self-Attention (MHSA)

CÆ¡ cháº¿ *attention* cho phÃ©p mÃ´ hÃ¬nh xÃ¡c Ä‘á»‹nh má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong chuá»—i.
Vá»›i **Multi-Head Attention**, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c nhiá»u kiá»ƒu quan há»‡ ngá»¯ nghÄ©a khÃ¡c nhau cÃ¹ng lÃºc.

CÃ´ng thá»©c tÃ­nh Attention cÆ¡ báº£n:

[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
]

Trong Ä‘Ã³:

* ( Q ): Query
* ( K ): Key
* ( V ): Value
* ( d_k ): KÃ­ch thÆ°á»›c khÃ´ng gian cá»§a vector key

#### âš™ï¸ Feed Forward Network (FFN)

Sau lá»›p attention, má»—i token Ä‘Æ°á»£c Ä‘Æ°a qua má»™t máº¡ng FFN hai táº§ng Ä‘á»ƒ tÄƒng kháº£ nÄƒng biá»ƒu diá»…n phi tuyáº¿n.

#### ğŸ” Residual Connection & Layer Normalization

GiÃºp mÃ´ hÃ¬nh huáº¥n luyá»‡n á»•n Ä‘á»‹nh, trÃ¡nh hiá»‡n tÆ°á»£ng máº¥t gradient.

---

## 3. Biá»ƒu diá»…n dá»¯ liá»‡u Ä‘áº§u vÃ o

### 3.1. Tokenization

VÄƒn báº£n Ä‘Æ°á»£c chia thÃ nh cÃ¡c **token** (Ä‘Æ¡n vá»‹ nhá» hÆ¡n tá»«).
CÃ¡c phÆ°Æ¡ng phÃ¡p phá»• biáº¿n:

* **BPE (Byte Pair Encoding)**
* **SentencePiece**
* **WordPiece**

### 3.2. Positional Encoding

Transformer khÃ´ng cÃ³ khÃ¡i niá»‡m tuáº§n tá»±, vÃ¬ váº­y **Positional Encoding** Ä‘Æ°á»£c thÃªm vÃ o embedding Ä‘á»ƒ cung cáº¥p thÃ´ng tin vá» vá»‹ trÃ­ token trong chuá»—i.

---

## 4. Huáº¥n luyá»‡n mÃ´ hÃ¬nh

### 4.1. Objective Function

Má»¥c tiÃªu phá»• biáº¿n lÃ  **Language Modeling Objective**:

[
\mathcal{L} = - \sum_{t} \log P(x_t | x_{<t})
]

MÃ´ hÃ¬nh há»c cÃ¡ch **dá»± Ä‘oÃ¡n token káº¿ tiáº¿p** dá»±a trÃªn cÃ¡c token trÆ°á»›c Ä‘Ã³.

### 4.2. Pre-training vÃ  Fine-tuning

* **Pre-training**: Huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u khá»•ng lá»“, khÃ´ng gÃ¡n nhÃ£n.
* **Fine-tuning**: Äiá»u chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ (dá»‹ch, há»i Ä‘Ã¡p, tÃ³m táº¯t,...).
* **Instruction Tuning / RLHF** (Reinforcement Learning from Human Feedback): GiÃºp mÃ´ hÃ¬nh pháº£n há»“i phÃ¹ há»£p hÆ¡n vá»›i con ngÆ°á»i.

---

## 5. Háº¡ táº§ng vÃ  tá»‘i Æ°u hÃ³a

### 5.1. Pháº§n cá»©ng

LLM cáº§n **GPU/TPU** hiá»‡u nÄƒng cao, bá»™ nhá»› lá»›n (vÃ i trÄƒm GB VRAM).
CÃ¡c mÃ´ hÃ¬nh cá»±c lá»›n (vÃ­ dá»¥ GPT-4) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn **hÃ ng nghÃ¬n GPU song song**.

### 5.2. Ká»¹ thuáº­t tá»‘i Æ°u

Má»™t sá»‘ ká»¹ thuáº­t Ä‘á»ƒ huáº¥n luyá»‡n LLM hiá»‡u quáº£ hÆ¡n:

* **Mixed Precision Training (FP16/BF16)**
* **Model Parallelism / Tensor Parallelism / Pipeline Parallelism**
* **Gradient Checkpointing**
* **ZeRO Optimization** (DeepSpeed)

---

## 6. Suy luáº­n (Inference) vÃ  nÃ©n mÃ´ hÃ¬nh

### 6.1. Ká»¹ thuáº­t suy luáº­n

* **Greedy Search**
* **Beam Search**
* **Top-k / Top-p (Nucleus) Sampling**

### 6.2. NÃ©n mÃ´ hÃ¬nh

Äá»ƒ giáº£m chi phÃ­ triá»ƒn khai:

* **Quantization**: Giáº£m Ä‘á»™ chÃ­nh xÃ¡c trá»ng sá»‘ (tá»« FP32 â†’ INT8/4-bit)
* **Pruning**: Loáº¡i bá» káº¿t ná»‘i hoáº·c neuron Ã­t quan trá»ng
* **Knowledge Distillation**: Huáº¥n luyá»‡n mÃ´ hÃ¬nh nhá» â€œbáº¯t chÆ°á»›câ€ mÃ´ hÃ¬nh lá»›n

---

## 7. Xu hÆ°á»›ng phÃ¡t triá»ƒn

* **Mixture of Experts (MoE)**: MÃ´ hÃ¬nh chia thÃ nh nhiá»u chuyÃªn gia, chá»‰ kÃ­ch hoáº¡t má»™t pháº§n trong má»—i lÆ°á»£t suy luáº­n â†’ tiáº¿t kiá»‡m tÃ i nguyÃªn.
* **Retrieval-Augmented Generation (RAG)**: Káº¿t há»£p LLM vá»›i há»‡ thá»‘ng tÃ¬m kiáº¿m thÃ´ng tin.
* **Multimodal LLMs**: Xá»­ lÃ½ vÃ  sinh ná»™i dung tá»« nhiá»u loáº¡i dá»¯ liá»‡u (vÄƒn báº£n, hÃ¬nh áº£nh, Ã¢m thanh, video).

---

## ğŸ“š Káº¿t luáº­n

LLMs lÃ  sá»± káº¿t há»£p giá»¯a:

* Kiáº¿n trÃºc **Transformer máº¡nh máº½**
* Dá»¯ liá»‡u huáº¥n luyá»‡n khá»•ng lá»“
* Tá»‘i Æ°u pháº§n cá»©ng tiÃªn tiáº¿n

Viá»‡c hiá»ƒu rÃµ **thÃ nh pháº§n ká»¹ thuáº­t** giÃºp ta khÃ´ng chá»‰ sá»­ dá»¥ng mÃ  cÃ²n **tÃ¹y chá»‰nh, tá»‘i Æ°u vÃ  triá»ƒn khai LLM hiá»‡u quáº£ hÆ¡n** trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿.

---

Báº¡n cÃ³ muá»‘n mÃ¬nh má»Ÿ rá»™ng bÃ i nÃ y thÃ nh **bÃ¡o cÃ¡o ká»¹ thuáº­t dáº¡ng PDF** (cÃ³ hÃ¬nh minh há»a kiáº¿n trÃºc Transformer vÃ  pipeline huáº¥n luyá»‡n) khÃ´ng? MÃ¬nh cÃ³ thá»ƒ táº¡o giÃºp ngay.